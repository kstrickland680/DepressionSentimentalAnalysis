{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\Users\\kas2n\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m   \u001b[1;31m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-9b46a0df4d9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;31m# go/tf-wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_pywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[1;33m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[1;32m---> 83\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Users\\kas2n\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "import string\n",
    "\n",
    "import re\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kas2n\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kas2n\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kas2n\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/sentiment_tweets3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>message to examine</th>\n",
       "      <th>label (depression result)</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>106</td>\n",
       "      <td>just had a real good moment. i missssssssss hi...</td>\n",
       "      <td>0</td>\n",
       "      <td>[just, had, a, real, good, moment, ., i, misss...</td>\n",
       "      <td>just had a real good moment. i missssssssss hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>217</td>\n",
       "      <td>is reading manga  http://plurk.com/p/mzp1e</td>\n",
       "      <td>0</td>\n",
       "      <td>[is, reading, manga]</td>\n",
       "      <td>is reading manga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>220</td>\n",
       "      <td>@comeagainjen http://twitpic.com/2y2lx - http:...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-]</td>\n",
       "      <td>@comeagainjen -</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>288</td>\n",
       "      <td>@lapcat Need to send 'em to my accountant tomo...</td>\n",
       "      <td>0</td>\n",
       "      <td>[need, to, send, ', em, to, my, accountant, to...</td>\n",
       "      <td>@lapcat need to send 'em to my accountant tomo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>540</td>\n",
       "      <td>ADD ME ON MYSPACE!!!  myspace.com/LookThunder</td>\n",
       "      <td>0</td>\n",
       "      <td>[add, me, on, myspace, !, !, !]</td>\n",
       "      <td>add me on myspace!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>624</td>\n",
       "      <td>so sleepy. good times tonight though</td>\n",
       "      <td>0</td>\n",
       "      <td>[so, sleepy, ., good, times, tonight, though]</td>\n",
       "      <td>so sleepy. good times tonight though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>701</td>\n",
       "      <td>@SilkCharm re: #nbn as someone already said, d...</td>\n",
       "      <td>0</td>\n",
       "      <td>[re, :, #nbn, as, someone, already, said, ,, d...</td>\n",
       "      <td>@silkcharm re: #nbn as someone already said, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>808</td>\n",
       "      <td>23 or 24ï¿½C possible today. Nice</td>\n",
       "      <td>0</td>\n",
       "      <td>[23, or, 24ï, ¿, ½, c, possible, today, ., nice]</td>\n",
       "      <td>23 or 24ï¿½c possible today. nice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1193</td>\n",
       "      <td>nite twitterville  workout in the am  -ciao</td>\n",
       "      <td>0</td>\n",
       "      <td>[nite, twitterville, workout, in, the, am, -, ...</td>\n",
       "      <td>nite twitterville workout in the am -ciao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1324</td>\n",
       "      <td>@daNanner Night, darlin'!  Sweet dreams to you</td>\n",
       "      <td>0</td>\n",
       "      <td>[night, ,, darlin, ', !, sweet, dreams, to, you]</td>\n",
       "      <td>@dananner night, darlin'! sweet dreams to you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1332</td>\n",
       "      <td>Good morning everybody!</td>\n",
       "      <td>0</td>\n",
       "      <td>[good, morning, everybody, !]</td>\n",
       "      <td>good morning everybody!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1368</td>\n",
       "      <td>Finally! I just created my WordPress Blog. The...</td>\n",
       "      <td>0</td>\n",
       "      <td>[finally, !, i, just, created, my, wordpress, ...</td>\n",
       "      <td>finally! i just created my wordpress blog. the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1578</td>\n",
       "      <td>kisha they cnt get over u til they get out frm...</td>\n",
       "      <td>0</td>\n",
       "      <td>[kisha, they, cnt, get, over, u, til, they, ge...</td>\n",
       "      <td>kisha they cnt get over u til they get out frm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1595</td>\n",
       "      <td>@nicolerichie Yes i remember that band, It was...</td>\n",
       "      <td>0</td>\n",
       "      <td>[yes, i, remember, that, band, ,, it, was, awe...</td>\n",
       "      <td>@nicolerichie yes i remember that band, it was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1861</td>\n",
       "      <td>I really love reflections and shadows</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, really, love, reflections, and, shadows]</td>\n",
       "      <td>i really love reflections and shadows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1889</td>\n",
       "      <td>@blueaero ooo it's fantasy?  i like fantasy no...</td>\n",
       "      <td>0</td>\n",
       "      <td>[ooo, it's, fantasy, ?, i, like, fantasy, nove...</td>\n",
       "      <td>@blueaero ooo it's fantasy? i like fantasy nov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1899</td>\n",
       "      <td>@rokchic28 no probs, I sell nothing other than...</td>\n",
       "      <td>0</td>\n",
       "      <td>[no, probs, ,, i, sell, nothing, other, than, ...</td>\n",
       "      <td>@rokchic28 no probs, i sell nothing other than...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1919</td>\n",
       "      <td>@shipovalov &amp;quot;NOKLA connecting people&amp;quot...</td>\n",
       "      <td>0</td>\n",
       "      <td>[\", nokla, connecting, people, \", ?, ?, ?, ?, ...</td>\n",
       "      <td>@shipovalov &amp;quot;nokla connecting people&amp;quot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1992</td>\n",
       "      <td>Once again stayed up to late and have to start...</td>\n",
       "      <td>0</td>\n",
       "      <td>[once, again, stayed, up, to, late, and, have,...</td>\n",
       "      <td>once again stayed up to late and have to start...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2097</td>\n",
       "      <td>@Kal_Penn I just read about your new job, CONG...</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, just, read, about, your, new, job, ,, cong...</td>\n",
       "      <td>@kal_penn i just read about your new job, cong...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Index                                 message to examine  \\\n",
       "0     106  just had a real good moment. i missssssssss hi...   \n",
       "1     217         is reading manga  http://plurk.com/p/mzp1e   \n",
       "2     220  @comeagainjen http://twitpic.com/2y2lx - http:...   \n",
       "3     288  @lapcat Need to send 'em to my accountant tomo...   \n",
       "4     540      ADD ME ON MYSPACE!!!  myspace.com/LookThunder   \n",
       "5     624              so sleepy. good times tonight though    \n",
       "6     701  @SilkCharm re: #nbn as someone already said, d...   \n",
       "7     808                 23 or 24ï¿½C possible today. Nice    \n",
       "8    1193        nite twitterville  workout in the am  -ciao   \n",
       "9    1324    @daNanner Night, darlin'!  Sweet dreams to you    \n",
       "10   1332                           Good morning everybody!    \n",
       "11   1368  Finally! I just created my WordPress Blog. The...   \n",
       "12   1578  kisha they cnt get over u til they get out frm...   \n",
       "13   1595  @nicolerichie Yes i remember that band, It was...   \n",
       "14   1861             I really love reflections and shadows    \n",
       "15   1889  @blueaero ooo it's fantasy?  i like fantasy no...   \n",
       "16   1899  @rokchic28 no probs, I sell nothing other than...   \n",
       "17   1919  @shipovalov &quot;NOKLA connecting people&quot...   \n",
       "18   1992  Once again stayed up to late and have to start...   \n",
       "19   2097  @Kal_Penn I just read about your new job, CONG...   \n",
       "\n",
       "    label (depression result)  \\\n",
       "0                           0   \n",
       "1                           0   \n",
       "2                           0   \n",
       "3                           0   \n",
       "4                           0   \n",
       "5                           0   \n",
       "6                           0   \n",
       "7                           0   \n",
       "8                           0   \n",
       "9                           0   \n",
       "10                          0   \n",
       "11                          0   \n",
       "12                          0   \n",
       "13                          0   \n",
       "14                          0   \n",
       "15                          0   \n",
       "16                          0   \n",
       "17                          0   \n",
       "18                          0   \n",
       "19                          0   \n",
       "\n",
       "                                            tokenized  \\\n",
       "0   [just, had, a, real, good, moment, ., i, misss...   \n",
       "1                                [is, reading, manga]   \n",
       "2                                                 [-]   \n",
       "3   [need, to, send, ', em, to, my, accountant, to...   \n",
       "4                     [add, me, on, myspace, !, !, !]   \n",
       "5       [so, sleepy, ., good, times, tonight, though]   \n",
       "6   [re, :, #nbn, as, someone, already, said, ,, d...   \n",
       "7    [23, or, 24ï, ¿, ½, c, possible, today, ., nice]   \n",
       "8   [nite, twitterville, workout, in, the, am, -, ...   \n",
       "9    [night, ,, darlin, ', !, sweet, dreams, to, you]   \n",
       "10                      [good, morning, everybody, !]   \n",
       "11  [finally, !, i, just, created, my, wordpress, ...   \n",
       "12  [kisha, they, cnt, get, over, u, til, they, ge...   \n",
       "13  [yes, i, remember, that, band, ,, it, was, awe...   \n",
       "14       [i, really, love, reflections, and, shadows]   \n",
       "15  [ooo, it's, fantasy, ?, i, like, fantasy, nove...   \n",
       "16  [no, probs, ,, i, sell, nothing, other, than, ...   \n",
       "17  [\", nokla, connecting, people, \", ?, ?, ?, ?, ...   \n",
       "18  [once, again, stayed, up, to, late, and, have,...   \n",
       "19  [i, just, read, about, your, new, job, ,, cong...   \n",
       "\n",
       "                                           lemmatized  \n",
       "0   just had a real good moment. i missssssssss hi...  \n",
       "1                                    is reading manga  \n",
       "2                                     @comeagainjen -  \n",
       "3   @lapcat need to send 'em to my accountant tomo...  \n",
       "4                                add me on myspace!!!  \n",
       "5                so sleepy. good times tonight though  \n",
       "6   @silkcharm re: #nbn as someone already said, d...  \n",
       "7                   23 or 24ï¿½c possible today. nice  \n",
       "8           nite twitterville workout in the am -ciao  \n",
       "9       @dananner night, darlin'! sweet dreams to you  \n",
       "10                            good morning everybody!  \n",
       "11  finally! i just created my wordpress blog. the...  \n",
       "12  kisha they cnt get over u til they get out frm...  \n",
       "13  @nicolerichie yes i remember that band, it was...  \n",
       "14              i really love reflections and shadows  \n",
       "15  @blueaero ooo it's fantasy? i like fantasy nov...  \n",
       "16  @rokchic28 no probs, i sell nothing other than...  \n",
       "17  @shipovalov &quot;nokla connecting people&quot...  \n",
       "18  once again stayed up to late and have to start...  \n",
       "19  @kal_penn i just read about your new job, cong...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.775645\n",
       "1    0.224355\n",
       "Name: label (depression result), dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label (depression result)'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10314, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10314, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates(inplace = True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index                        False\n",
       "message to examine           False\n",
       "label (depression result)    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['just',\n",
       "  'had',\n",
       "  'a',\n",
       "  'real',\n",
       "  'good',\n",
       "  'moment',\n",
       "  '.',\n",
       "  'i',\n",
       "  'misss',\n",
       "  'him',\n",
       "  'so',\n",
       "  'much',\n",
       "  ','],\n",
       " ['is', 'reading', 'manga', 'http://plurk.com/p/mzp1e'],\n",
       " ['http://twitpic.com/2y2lx',\n",
       "  '-',\n",
       "  'http://www.youtube.com/watch?v=zoGfqvh2ME8'],\n",
       " ['Need',\n",
       "  'to',\n",
       "  'send',\n",
       "  \"'\",\n",
       "  'em',\n",
       "  'to',\n",
       "  'my',\n",
       "  'accountant',\n",
       "  'tomorrow',\n",
       "  '.',\n",
       "  'Oddly',\n",
       "  ',',\n",
       "  'I',\n",
       "  \"wasn't\",\n",
       "  'even',\n",
       "  'referring',\n",
       "  'to',\n",
       "  'my',\n",
       "  'taxes',\n",
       "  '.',\n",
       "  'Those',\n",
       "  'are',\n",
       "  'supporting',\n",
       "  'evidence',\n",
       "  ',',\n",
       "  'though',\n",
       "  '.'],\n",
       " ['ADD', 'ME', 'ON', 'MYSPACE', '!', '!', '!', 'myspace.com/LookThunder'],\n",
       " ['so', 'sleepy', '.', 'good', 'times', 'tonight', 'though'],\n",
       " ['re',\n",
       "  ':',\n",
       "  '#nbn',\n",
       "  'as',\n",
       "  'someone',\n",
       "  'already',\n",
       "  'said',\n",
       "  ',',\n",
       "  'does',\n",
       "  'fiber',\n",
       "  'to',\n",
       "  'the',\n",
       "  'home',\n",
       "  'mean',\n",
       "  'we',\n",
       "  'will',\n",
       "  'all',\n",
       "  'at',\n",
       "  'least',\n",
       "  'be',\n",
       "  'regular',\n",
       "  'now'],\n",
       " ['23', 'or', '24ï', '¿', '½', 'C', 'possible', 'today', '.', 'Nice'],\n",
       " ['nite', 'twitterville', 'workout', 'in', 'the', 'am', '-', 'ciao'],\n",
       " ['Night', ',', 'darlin', \"'\", '!', 'Sweet', 'dreams', 'to', 'you'],\n",
       " ['Good', 'morning', 'everybody', '!'],\n",
       " ['Finally',\n",
       "  '!',\n",
       "  'I',\n",
       "  'just',\n",
       "  'created',\n",
       "  'my',\n",
       "  'WordPress',\n",
       "  'Blog',\n",
       "  '.',\n",
       "  \"There's\",\n",
       "  'already',\n",
       "  'a',\n",
       "  'blog',\n",
       "  'up',\n",
       "  'on',\n",
       "  'the',\n",
       "  'Seattle',\n",
       "  'Coffee',\n",
       "  'Community',\n",
       "  '...',\n",
       "  'http://tinyurl.com/c5uufd'],\n",
       " ['kisha',\n",
       "  'they',\n",
       "  'cnt',\n",
       "  'get',\n",
       "  'over',\n",
       "  'u',\n",
       "  'til',\n",
       "  'they',\n",
       "  'get',\n",
       "  'out',\n",
       "  'frm',\n",
       "  'under',\n",
       "  'u',\n",
       "  'just',\n",
       "  'remember',\n",
       "  'ur',\n",
       "  'on',\n",
       "  'top'],\n",
       " ['Yes',\n",
       "  'i',\n",
       "  'remember',\n",
       "  'that',\n",
       "  'band',\n",
       "  ',',\n",
       "  'It',\n",
       "  'was',\n",
       "  'Awesome',\n",
       "  ',',\n",
       "  'Will',\n",
       "  'you',\n",
       "  'please',\n",
       "  'reply'],\n",
       " ['I', 'really', 'love', 'reflections', 'and', 'shadows'],\n",
       " ['ooo',\n",
       "  \"it's\",\n",
       "  'fantasy',\n",
       "  '?',\n",
       "  'i',\n",
       "  'like',\n",
       "  'fantasy',\n",
       "  'novels',\n",
       "  'will',\n",
       "  'check',\n",
       "  'it',\n",
       "  'out'],\n",
       " ['no',\n",
       "  'probs',\n",
       "  ',',\n",
       "  'I',\n",
       "  'sell',\n",
       "  'nothing',\n",
       "  'other',\n",
       "  'than',\n",
       "  'my',\n",
       "  'blog',\n",
       "  'http://snedwan.com',\n",
       "  \"I'll\",\n",
       "  'have',\n",
       "  'to',\n",
       "  'get',\n",
       "  'a',\n",
       "  'listen',\n",
       "  'to',\n",
       "  'your',\n",
       "  'band',\n",
       "  ',',\n",
       "  'on',\n",
       "  'iTunes',\n",
       "  '?'],\n",
       " ['\"',\n",
       "  'NOKLA',\n",
       "  'connecting',\n",
       "  'people',\n",
       "  '\"',\n",
       "  '?',\n",
       "  '?',\n",
       "  '?',\n",
       "  '?',\n",
       "  '?',\n",
       "  '?',\n",
       "  '?',\n",
       "  '?'],\n",
       " ['Once',\n",
       "  'again',\n",
       "  'stayed',\n",
       "  'up',\n",
       "  'to',\n",
       "  'late',\n",
       "  'and',\n",
       "  'have',\n",
       "  'to',\n",
       "  'start',\n",
       "  'too',\n",
       "  'early',\n",
       "  'It',\n",
       "  'is',\n",
       "  'a',\n",
       "  'good',\n",
       "  'thing',\n",
       "  'I',\n",
       "  'like',\n",
       "  'my',\n",
       "  'job'],\n",
       " ['I',\n",
       "  'just',\n",
       "  'read',\n",
       "  'about',\n",
       "  'your',\n",
       "  'new',\n",
       "  'job',\n",
       "  ',',\n",
       "  'CONGRATULATIONS',\n",
       "  '!',\n",
       "  \"That's\",\n",
       "  'fantastic',\n",
       "  '.']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet_tokenizer = TweetTokenizer(strip_handles = True, reduce_len=True)\n",
    "\n",
    "tweet_tokens = []\n",
    "for sent in df['message to examine']:\n",
    "    tweet_tokens.append(tweet_tokenizer.tokenize(sent))\n",
    "    \n",
    "tweet_tokens[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is reading manga'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    test = 'is reading manga http://plurk.com/p/mzp1e'\n",
    "    tweett = ' '.join(re.sub(\"http://\\S+\", \" \", test).split())\n",
    "    tweett\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(tweet):\n",
    "    #remove numbers\n",
    "    #text_nonum = re.sub(r'\\d+', '', tweet)\n",
    "    #remove hashtags\n",
    "    text_nonum = re.sub('#', \"\", tweet)\n",
    "    #remove punctuations and convert characters to lower case\n",
    "    text_nopunct = \"\".join([char.lower() for char in text_nonum if char not in string.punctuation])\n",
    "    #remove multiple whitespace and leading/trailing whitespace\n",
    "    text_no_doublespace = re.sub('\\s+', ' ', text_nopunct).strip()\n",
    "    return text_no_doublespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nopunct(tweet):\n",
    "    new_words = [word for word in tweet if word.isalnum()]\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>message to examine</th>\n",
       "      <th>label (depression result)</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>106</td>\n",
       "      <td>just had a real good moment. i missssssssss hi...</td>\n",
       "      <td>0</td>\n",
       "      <td>[just, had, a, real, good, moment, i, misss, h...</td>\n",
       "      <td>just had a real good moment. i missssssssss hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>217</td>\n",
       "      <td>is reading manga  http://plurk.com/p/mzp1e</td>\n",
       "      <td>0</td>\n",
       "      <td>[is, reading, manga]</td>\n",
       "      <td>is reading manga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>220</td>\n",
       "      <td>@comeagainjen http://twitpic.com/2y2lx - http:...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>@comeagainjen -</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>288</td>\n",
       "      <td>@lapcat Need to send 'em to my accountant tomo...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Need, to, send, em, to, my, accountant, tomor...</td>\n",
       "      <td>@lapcat Need to send 'em to my accountant tomo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>540</td>\n",
       "      <td>ADD ME ON MYSPACE!!!  myspace.com/LookThunder</td>\n",
       "      <td>0</td>\n",
       "      <td>[ADD, ME, ON, MYSPACE]</td>\n",
       "      <td>ADD ME ON MYSPACE!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>624</td>\n",
       "      <td>so sleepy. good times tonight though</td>\n",
       "      <td>0</td>\n",
       "      <td>[so, sleepy, good, times, tonight, though]</td>\n",
       "      <td>so sleepy. good times tonight though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>701</td>\n",
       "      <td>@SilkCharm re: #nbn as someone already said, d...</td>\n",
       "      <td>0</td>\n",
       "      <td>[re, nbn, as, someone, already, said, does, fi...</td>\n",
       "      <td>@SilkCharm re: nbn as someone already said, do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>808</td>\n",
       "      <td>23 or 24ï¿½C possible today. Nice</td>\n",
       "      <td>0</td>\n",
       "      <td>[or, ï, ½, C, possible, today, Nice]</td>\n",
       "      <td>or ï¿½C possible today. Nice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1193</td>\n",
       "      <td>nite twitterville  workout in the am  -ciao</td>\n",
       "      <td>0</td>\n",
       "      <td>[nite, twitterville, workout, in, the, am, ciao]</td>\n",
       "      <td>nite twitterville workout in the am -ciao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1324</td>\n",
       "      <td>@daNanner Night, darlin'!  Sweet dreams to you</td>\n",
       "      <td>0</td>\n",
       "      <td>[Night, darlin, Sweet, dreams, to, you]</td>\n",
       "      <td>@daNanner Night, darlin'! Sweet dreams to you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1332</td>\n",
       "      <td>Good morning everybody!</td>\n",
       "      <td>0</td>\n",
       "      <td>[Good, morning, everybody]</td>\n",
       "      <td>Good morning everybody!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1368</td>\n",
       "      <td>Finally! I just created my WordPress Blog. The...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Finally, I, just, created, my, WordPress, Blo...</td>\n",
       "      <td>Finally! I just created my WordPress Blog. The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1578</td>\n",
       "      <td>kisha they cnt get over u til they get out frm...</td>\n",
       "      <td>0</td>\n",
       "      <td>[kisha, they, cnt, get, over, u, til, they, ge...</td>\n",
       "      <td>kisha they cnt get over u til they get out frm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1595</td>\n",
       "      <td>@nicolerichie Yes i remember that band, It was...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Yes, i, remember, that, band, It, was, Awesom...</td>\n",
       "      <td>@nicolerichie Yes i remember that band, It was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1861</td>\n",
       "      <td>I really love reflections and shadows</td>\n",
       "      <td>0</td>\n",
       "      <td>[I, really, love, reflections, and, shadows]</td>\n",
       "      <td>I really love reflections and shadows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1889</td>\n",
       "      <td>@blueaero ooo it's fantasy?  i like fantasy no...</td>\n",
       "      <td>0</td>\n",
       "      <td>[ooo, fantasy, i, like, fantasy, novels, will,...</td>\n",
       "      <td>@blueaero ooo it's fantasy? i like fantasy nov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1899</td>\n",
       "      <td>@rokchic28 no probs, I sell nothing other than...</td>\n",
       "      <td>0</td>\n",
       "      <td>[no, probs, I, sell, nothing, other, than, my,...</td>\n",
       "      <td>@rokchic no probs, I sell nothing other than m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1919</td>\n",
       "      <td>@shipovalov &amp;quot;NOKLA connecting people&amp;quot...</td>\n",
       "      <td>0</td>\n",
       "      <td>[NOKLA, connecting, people]</td>\n",
       "      <td>@shipovalov &amp;quot;NOKLA connecting people&amp;quot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1992</td>\n",
       "      <td>Once again stayed up to late and have to start...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Once, again, stayed, up, to, late, and, have,...</td>\n",
       "      <td>Once again stayed up to late and have to start...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2097</td>\n",
       "      <td>@Kal_Penn I just read about your new job, CONG...</td>\n",
       "      <td>0</td>\n",
       "      <td>[I, just, read, about, your, new, job, CONGRAT...</td>\n",
       "      <td>@Kal_Penn I just read about your new job, CONG...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Index                                 message to examine  \\\n",
       "0     106  just had a real good moment. i missssssssss hi...   \n",
       "1     217         is reading manga  http://plurk.com/p/mzp1e   \n",
       "2     220  @comeagainjen http://twitpic.com/2y2lx - http:...   \n",
       "3     288  @lapcat Need to send 'em to my accountant tomo...   \n",
       "4     540      ADD ME ON MYSPACE!!!  myspace.com/LookThunder   \n",
       "5     624              so sleepy. good times tonight though    \n",
       "6     701  @SilkCharm re: #nbn as someone already said, d...   \n",
       "7     808                 23 or 24ï¿½C possible today. Nice    \n",
       "8    1193        nite twitterville  workout in the am  -ciao   \n",
       "9    1324    @daNanner Night, darlin'!  Sweet dreams to you    \n",
       "10   1332                           Good morning everybody!    \n",
       "11   1368  Finally! I just created my WordPress Blog. The...   \n",
       "12   1578  kisha they cnt get over u til they get out frm...   \n",
       "13   1595  @nicolerichie Yes i remember that band, It was...   \n",
       "14   1861             I really love reflections and shadows    \n",
       "15   1889  @blueaero ooo it's fantasy?  i like fantasy no...   \n",
       "16   1899  @rokchic28 no probs, I sell nothing other than...   \n",
       "17   1919  @shipovalov &quot;NOKLA connecting people&quot...   \n",
       "18   1992  Once again stayed up to late and have to start...   \n",
       "19   2097  @Kal_Penn I just read about your new job, CONG...   \n",
       "\n",
       "    label (depression result)  \\\n",
       "0                           0   \n",
       "1                           0   \n",
       "2                           0   \n",
       "3                           0   \n",
       "4                           0   \n",
       "5                           0   \n",
       "6                           0   \n",
       "7                           0   \n",
       "8                           0   \n",
       "9                           0   \n",
       "10                          0   \n",
       "11                          0   \n",
       "12                          0   \n",
       "13                          0   \n",
       "14                          0   \n",
       "15                          0   \n",
       "16                          0   \n",
       "17                          0   \n",
       "18                          0   \n",
       "19                          0   \n",
       "\n",
       "                                            tokenized  \\\n",
       "0   [just, had, a, real, good, moment, i, misss, h...   \n",
       "1                                [is, reading, manga]   \n",
       "2                                                  []   \n",
       "3   [Need, to, send, em, to, my, accountant, tomor...   \n",
       "4                              [ADD, ME, ON, MYSPACE]   \n",
       "5          [so, sleepy, good, times, tonight, though]   \n",
       "6   [re, nbn, as, someone, already, said, does, fi...   \n",
       "7                [or, ï, ½, C, possible, today, Nice]   \n",
       "8    [nite, twitterville, workout, in, the, am, ciao]   \n",
       "9             [Night, darlin, Sweet, dreams, to, you]   \n",
       "10                         [Good, morning, everybody]   \n",
       "11  [Finally, I, just, created, my, WordPress, Blo...   \n",
       "12  [kisha, they, cnt, get, over, u, til, they, ge...   \n",
       "13  [Yes, i, remember, that, band, It, was, Awesom...   \n",
       "14       [I, really, love, reflections, and, shadows]   \n",
       "15  [ooo, fantasy, i, like, fantasy, novels, will,...   \n",
       "16  [no, probs, I, sell, nothing, other, than, my,...   \n",
       "17                        [NOKLA, connecting, people]   \n",
       "18  [Once, again, stayed, up, to, late, and, have,...   \n",
       "19  [I, just, read, about, your, new, job, CONGRAT...   \n",
       "\n",
       "                                           lemmatized  \n",
       "0   just had a real good moment. i missssssssss hi...  \n",
       "1                                    is reading manga  \n",
       "2                                     @comeagainjen -  \n",
       "3   @lapcat Need to send 'em to my accountant tomo...  \n",
       "4                                ADD ME ON MYSPACE!!!  \n",
       "5                so sleepy. good times tonight though  \n",
       "6   @SilkCharm re: nbn as someone already said, do...  \n",
       "7                        or ï¿½C possible today. Nice  \n",
       "8           nite twitterville workout in the am -ciao  \n",
       "9       @daNanner Night, darlin'! Sweet dreams to you  \n",
       "10                            Good morning everybody!  \n",
       "11  Finally! I just created my WordPress Blog. The...  \n",
       "12  kisha they cnt get over u til they get out frm...  \n",
       "13  @nicolerichie Yes i remember that band, It was...  \n",
       "14              I really love reflections and shadows  \n",
       "15  @blueaero ooo it's fantasy? i like fantasy nov...  \n",
       "16  @rokchic no probs, I sell nothing other than m...  \n",
       "17  @shipovalov &quot;NOKLA connecting people&quot...  \n",
       "18  Once again stayed up to late and have to start...  \n",
       "19  @Kal_Penn I just read about your new job, CONG...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#def tokenize_tweets(tweets):\n",
    "\n",
    "    \n",
    "#remove URLS:\n",
    "df['tokenized'] = [' '.join(re.sub(\"http://\\S+\", \" \", tweet).split()) for tweet in df['message to examine']]\n",
    "df['tokenized'] = [' '.join(re.sub(\"myspace.com\\S+\", \" \", tweet).split()) for tweet in df['tokenized']]\n",
    "#df['tokenized'] = [tweet.lower() for tweet in df['tokenized']]\n",
    "\n",
    "\n",
    "#for tweet in df['tokenized']:\n",
    " #   df['tokenized'] = df['tokenized'].''.join([c for c in tweet if c not in punct])\n",
    "\n",
    "#remove numbers\n",
    "df['tokenized'] = [' '.join(re.sub(r'\\d+', '', tweet).split()) for tweet in df['tokenized']]\n",
    "\n",
    "#remove hashtag\n",
    "df['tokenized'] = [' '.join(re.sub('#', \"\", tweet).split()) for tweet in df['tokenized']]\n",
    "#remove punctuation\n",
    "\n",
    "#df['tokenized'] = [\"\".join([char.lower() for char in df['tokenized'] if char not in string.punctuation])]\n",
    "\n",
    "#df['tokenized'] = [clean_text(tweet) for tweet in df['tokenized']]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['lemmatized'] = [lemmatizer.lemmatize(word) for word in df['tokenized']]\n",
    "\n",
    "\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer(strip_handles = True, reduce_len=True)\n",
    "df['tokenized'] = [tweet_tokenizer.tokenize(entry) for entry in df['lemmatized']]\n",
    "  \n",
    "df['tokenized'] = [nopunct(tweet) for tweet in df['tokenized']]\n",
    "\n",
    "#df['tokenized'] = [clean_text(tweet) for tweet in df['tokenized']]\n",
    "\n",
    "#pun = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "#df['tokenized'] = df['tokenized'].map(pun.tokenize)\n",
    "#need to remove www...\n",
    "\n",
    "#remove punctuation\n",
    "\n",
    "\n",
    "\n",
    "df.head(20)\n",
    "    #training a byte-wise bpe tokenizer --> huggingface\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df['tokenized']\n",
    "target = df['label (depression result)']\n",
    "model = Word2Vec(data, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-4ddf23955dbf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label (depression result)'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdepressed_freqdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdep\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokenized'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdepressed_freqdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\probability.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, samples)\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \"\"\"\n\u001b[1;32m--> 104\u001b[1;33m         \u001b[0mCounter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;31m# Cached number of samples in this FreqDist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\collections\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, iterable, **kwds)\u001b[0m\n\u001b[0;32m    550\u001b[0m         '''\n\u001b[0;32m    551\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__missing__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\probability.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \"\"\"\n\u001b[0;32m    141\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_N\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\collections\\__init__.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, iterable, **kwds)\u001b[0m\n\u001b[0;32m    635\u001b[0m                     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# fast path when counter is empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 637\u001b[1;33m                 \u001b[0m_count_elements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    638\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "dep = df[df['label (depression result)'] == 1]\n",
    "depressed_freqdist = FreqDist(dep['tokenized'])\n",
    "depressed_freqdist.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1157939, 1424610)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(data, total_examples=model.corpus_count, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('someone', 0.9486861228942871),\n",
       " ('things', 0.9418240785598755),\n",
       " ('who', 0.9322532415390015),\n",
       " ('him', 0.9278783202171326),\n",
       " ('they', 0.9197596907615662),\n",
       " ('something', 0.9148552417755127),\n",
       " ('demand', 0.9143405556678772),\n",
       " ('us', 0.908324122428894),\n",
       " ('none', 0.9056959748268127),\n",
       " ('them', 0.9048210382461548)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar('people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vocab = set(word for tweet in data for word in tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18375"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = {}\n",
    "with open('glove.twitter.27B.50d.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode('utf-8')\n",
    "        if word in total_vocab:\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.3871  ,  0.073452,  0.71934 , -0.13666 ,  0.44342 ,  0.12125 ,\n",
       "        1.5426  ,  0.69879 ,  0.64324 , -0.092814, -0.024592,  0.31461 ,\n",
       "       -3.3482  , -0.15844 ,  0.26598 , -0.24607 , -0.31773 ,  0.10577 ,\n",
       "       -0.14858 ,  0.13287 , -0.26136 ,  1.1169  , -0.6378  , -0.36226 ,\n",
       "        0.28108 ,  0.34938 , -0.09388 , -0.18142 , -0.334   , -0.17178 ,\n",
       "        0.28481 , -0.44815 ,  0.46787 , -0.016544,  1.0707  , -0.80139 ,\n",
       "        0.59951 , -0.66364 ,  0.083816, -0.011978, -1.7931  ,  0.44126 ,\n",
       "       -1.2955  ,  0.18929 , -1.2393  ,  0.35362 , -0.089281, -0.16474 ,\n",
       "       -0.18972 , -0.79012 ], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove['band']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2vVectorizer(object):\n",
    "    def __init__(self, w2v):\n",
    "        #takes in a dictionary of words and vectors as input\n",
    "        self.w2v = w2v\n",
    "        if len(w2v) == 0:\n",
    "            self.dimensions = 0\n",
    "        else:\n",
    "            self.dimensions = len(w2v[next(iter(glove))])\n",
    "    \n",
    "    # Note: Even though it doesn't do anything, it's required that this object implement a fit method or else\n",
    "    # it can't be used in a scikit-learn pipeline  \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.w2v[w] for w in words if w in self.w2v]\n",
    "                   or [np.zeros(self.dimensions)], axis=0) for words in X])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf =  Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "              ('Random Forest', RandomForestClassifier(n_estimators=100, verbose=True))])\n",
    "svc = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "                ('Support Vector Machine', SVC())])\n",
    "lr = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "              ('Logistic Regression', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [('Random Forest', rf),\n",
    "          ('Support Vector Machine', svc),\n",
    "          ('Logistic Regression', lr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "scores = [(name, cross_val_score(model, data, target, cv=2).mean()) for name, model, in models]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Random Forest', 0.9265076594919527),\n",
       " ('Support Vector Machine', 0.9393057979445414),\n",
       " ('Logistic Regression', 0.9299980608881133)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m   \u001b[1;31m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_pywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     82\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[1;32m---> 83\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Users\\kas2n\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-21dd1344d01a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGlobalMaxPool1D\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minitializers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregularizers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     raise ImportError(\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[1;34m'Keras requires TensorFlow 2.2 or higher. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         'Install TensorFlow via `pip install tensorflow`')\n",
      "\u001b[1;31mImportError\u001b[0m: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, LSTM, Embedding\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Sequential\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.preprocessing import text, sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
